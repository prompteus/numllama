{
 "cells": [
  {
   "cell_type": "code",
   "id": "7a542467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:54:33.876099Z",
     "start_time": "2025-07-02T13:54:33.873052Z"
    }
   },
   "source": "device = \"cuda:3\"",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "1b9c875c",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b08b50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:54:36.042985Z",
     "start_time": "2025-07-02T13:54:34.008341Z"
    }
   },
   "source": [
    "import itertools\n",
    "import random\n",
    "import collections\n",
    "\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import tqdm.auto\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "da5bca2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:54:36.083201Z",
     "start_time": "2025-07-02T13:54:36.076893Z"
    }
   },
   "source": [
    "def sinusoidal_encode(\n",
    "    x: Tensor,\n",
    "    embedding_dim: int,\n",
    "    min_value: int,\n",
    "    max_value: int,\n",
    "    use_l2_norm: bool = False,\n",
    "    norm_const: float | None = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Encodes a tensor of numbers into a sinusoidal representation, inspired by how absolute positional\n",
    "    encoding works in transformers.\n",
    "\n",
    "    The encoding is an evaluation of a sine and cosine function at different frequencies, where the\n",
    "    frequency is determined by the embedding dimension and the allowed range of the input values.\n",
    "\n",
    "    >>> sinusoidal_encode(\n",
    "    ...     torch.tensor([-5, 2, 1, 0]),\n",
    "    ...     embedding_dim=6,\n",
    "    ...     min_value=-5,\n",
    "    ...     max_value=5,\n",
    "    ... )\n",
    "    tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
    "            [ 0.6570,  0.7539, -0.1073, -0.9942,  0.9980,  0.0627],\n",
    "            [-0.2794,  0.9602,  0.3491, -0.9371,  0.9616,  0.2746],\n",
    "            [-0.9589,  0.2837,  0.7317, -0.6816,  0.8806,  0.4738]])\n",
    "    \"\"\"\n",
    "\n",
    "    if embedding_dim % 2 != 0 and not use_l2_norm:\n",
    "        raise ValueError(\"Embedding dimension must be even\")\n",
    "\n",
    "    if use_l2_norm:\n",
    "        if embedding_dim % 2 == 0:\n",
    "            reserved_dim = 2\n",
    "        else:\n",
    "            reserved_dim = 1\n",
    "        embedding_dim -= reserved_dim\n",
    "    else:\n",
    "        reserved_dim = 0  # will not be used\n",
    "\n",
    "    domain = max_value - min_value\n",
    "    y_shape = x.shape + (embedding_dim,)\n",
    "    y = torch.zeros(y_shape, device=x.device)\n",
    "    even_indices = torch.arange(0, embedding_dim, 2)\n",
    "    log_term = torch.log(torch.tensor(domain)) / embedding_dim\n",
    "    div_term = torch.exp(even_indices * -log_term)\n",
    "    x = x - min_value\n",
    "    values = x.unsqueeze(-1).float() * div_term\n",
    "    y[..., 0::2] = torch.sin(values)\n",
    "    y[..., 1::2] = torch.cos(values)\n",
    "\n",
    "    if use_l2_norm:\n",
    "        y = torch.cat([y, torch.ones_like(y[..., :reserved_dim])], dim=-1)\n",
    "        y /= y.norm(dim=-1, keepdim=True, p=2)\n",
    "\n",
    "    if norm_const is not None:\n",
    "        y *= norm_const\n",
    "\n",
    "    return y"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "8ff97ee3",
   "metadata": {},
   "source": [
    "### Prepare model and data"
   ]
  },
  {
   "cell_type": "code",
   "id": "8723d3d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:54:41.350558Z",
     "start_time": "2025-07-02T13:54:36.468086Z"
    }
   },
   "source": [
    "model = transformers.AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B\").eval()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = model.to(device).eval()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "9b394470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:54:41.543063Z",
     "start_time": "2025-07-02T13:54:41.537367Z"
    }
   },
   "source": [
    "all_values = torch.arange(0, 1000)\n",
    "mask = torch.rand(len(all_values), generator=torch.Generator().manual_seed(0))\n",
    "train_mask = mask < 0.9\n",
    "valid_mask = ~train_mask & (mask < 0.95)\n",
    "test_mask = ~train_mask & ~valid_mask\n",
    "\n",
    "train_values = all_values[train_mask]\n",
    "valid_values = all_values[valid_mask]\n",
    "test_values = all_values[test_mask]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c41d1f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:54:41.692249Z",
     "start_time": "2025-07-02T13:54:41.681453Z"
    }
   },
   "source": [
    "def make_str_input(nums: list) -> str:\n",
    "    return str(nums[0]) + \"\".join(str(n).zfill(3) for n in nums[1:])\n",
    "\n",
    "make_str_input([3, 500, 789]), make_str_input([3, 0, 1])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3500789', '3000001')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "0d5ad909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:54:42.282681Z",
     "start_time": "2025-07-02T13:54:41.874728Z"
    }
   },
   "source": [
    "rng = random.Random(0)\n",
    "\n",
    "train_size = 1_000_000\n",
    "\n",
    "x_values_train = [(i, j) for i, j in zip(\n",
    "    rng.choices(train_values.tolist(), k=train_size),\n",
    "    rng.choices(train_values.tolist(), k=train_size)\n",
    ")]\n",
    "x_values_valid = list(itertools.product(valid_values.tolist(), repeat=2))\n",
    "x_values_test = list(itertools.product(test_values.tolist(), repeat=2))\n",
    "\n",
    "x_inputs_valid = tokenizer(list(map(make_str_input, x_values_valid)), return_tensors=\"pt\")\n",
    "x_inputs_valid = tokenizer(list(map(make_str_input, x_values_test)), return_tensors=\"pt\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "36dd2f3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:55:10.737968Z",
     "start_time": "2025-07-02T13:55:10.731714Z"
    }
   },
   "source": [
    "def get_hidden_states(model, str_inputs: list[str]) -> collections.defaultdict[int, Tensor]:\n",
    "    model.eval()\n",
    "    hidden_states = collections.defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for batch_str in itertools.batched(tqdm.auto.tqdm(str_inputs), n=128):\n",
    "            batch_inputs = tokenizer(batch_str, return_tensors=\"pt\")\n",
    "            hidden_reprs = model(**batch_inputs.to(model.device), output_hidden_states=True).hidden_states\n",
    "            layer_idx = 5\n",
    "            # for layer_idx, hidden_state in enumerate(hidden_reprs):\n",
    "            hidden_states[layer_idx].extend(hidden_reprs[layer_idx][:, -1, :].detach().cpu())\n",
    "    return {k: torch.stack(v).contiguous() for k, v in hidden_states.items()}"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "da87aca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:11:34.077645Z",
     "start_time": "2025-07-02T13:55:11.954394Z"
    }
   },
   "source": [
    "train_hidden_states = get_hidden_states(model, list(map(make_str_input, x_values_train)))\n",
    "valid_hidden_states = get_hidden_states(model, list(map(make_str_input, x_values_valid)))\n",
    "test_hidden_states = get_hidden_states(model, list(map(make_str_input, x_values_test)))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2469387b7ca4071b6748d1896058617"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3249 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afc98796a59b41a3a8b86118a0b328c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3025 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d4f853215af444ca052a923ae368133"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "6288a397",
   "metadata": {},
   "source": [
    "### Probing"
   ]
  },
  {
   "cell_type": "code",
   "id": "cff73d20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:12:00.539107Z",
     "start_time": "2025-07-02T14:12:00.528620Z"
    }
   },
   "source": [
    "basis_embs = sinusoidal_encode(\n",
    "    torch.arange(1000),\n",
    "    min_value=0,\n",
    "    max_value=1000,\n",
    "    embedding_dim=train_hidden_states[5].shape[-1],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "184992d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:12:08.783816Z",
     "start_time": "2025-07-02T14:12:08.771793Z"
    }
   },
   "source": [
    "class ClassifierProbe(torch.nn.Module):\n",
    "    def __init__(self, emb_dim: int, hidden_dim: int, basis: torch.Tensor, heldout_mask: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.emb_to_latent = torch.nn.Linear(emb_dim, hidden_dim, bias=True)\n",
    "        self.basis_to_latent = torch.nn.Linear(basis.shape[-1], hidden_dim, bias=True)\n",
    "        self.basis: torch.nn.Buffer\n",
    "        self.heldout_mask: torch.nn.Buffer\n",
    "        self.register_buffer(\"basis\", basis)\n",
    "        self.register_buffer(\"heldout_mask\", heldout_mask)\n",
    "    def forward(self, x: Tensor, holdout_eval_tokens: bool) -> Tensor:\n",
    "        latent_x = self.emb_to_latent(x)\n",
    "        # during training, model learns to choose among only training tokens\n",
    "        # but during eval, model must choose among all tokens\n",
    "        # this means that the model is never exposed to the eval tokens during training\n",
    "        latent_choices = self.basis_to_latent(self.basis)\n",
    "        logits = latent_x @ latent_choices.T\n",
    "        if holdout_eval_tokens:\n",
    "            logits[:, self.heldout_mask] = float(\"-inf\")\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "ed50b402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T15:15:19.887061Z",
     "start_time": "2025-07-02T15:14:13.869916Z"
    }
   },
   "source": [
    "torch.manual_seed(0)\n",
    "probe = ClassifierProbe(\n",
    "    emb_dim=train_hidden_states[5].shape[-1],\n",
    "    hidden_dim=100,\n",
    "    basis=basis_embs,\n",
    "    heldout_mask=test_mask,\n",
    ").to(device)\n",
    "\n",
    "layer_idx = 5 # choose the layer to probe\n",
    "optimizer = torch.optim.Adam(probe.parameters(), lr=1e-3)\n",
    "train_labels = torch.tensor([x[0] for x in x_values_train]) # we want to decode the first number token from hidden representation of the last token\n",
    "rng = torch.Generator().manual_seed(0)\n",
    "for i in range(10000):\n",
    "    probe.train()\n",
    "    optimizer.zero_grad()\n",
    "    minibatch_idcs = torch.randint(len(train_labels), size=(512,), generator=rng)\n",
    "    x = train_hidden_states[layer_idx][minibatch_idcs].to(device)\n",
    "    y = train_labels[minibatch_idcs].to(device)\n",
    "    logits = probe(x, holdout_eval_tokens=True)\n",
    "    # add l1 regularization of all params to the loss\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y) + 0.0003 * sum(p.abs().sum() for p in probe.parameters())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        train_acc = (logits.argmax(dim=-1) == y).float().mean()\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_logits = probe(valid_hidden_states[layer_idx].to(device), holdout_eval_tokens=False)\n",
    "            valid_labels = torch.tensor([x[0] for x in x_values_valid]).to(device)\n",
    "            valid_loss = torch.nn.functional.cross_entropy(valid_logits, valid_labels)\n",
    "            accuracy = (valid_logits.argmax(dim=-1) == valid_labels).float().mean()\n",
    "        print(f\"{i=:<5} train loss: {loss.item():.2f} train acc: {train_acc.item():.2f}  val loss: {valid_loss.item():.2f} valid acc: {accuracy.item():.2f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0     train loss: 8.23 train acc: 0.00  val loss: 6.96 valid acc: 0.00\n",
      "i=100   train loss: 2.99 train acc: 0.52  val loss: 2.44 valid acc: 0.10\n",
      "i=200   train loss: 2.04 train acc: 0.86  val loss: 1.74 valid acc: 0.39\n",
      "i=300   train loss: 1.63 train acc: 0.92  val loss: 1.40 valid acc: 0.61\n",
      "i=400   train loss: 1.39 train acc: 0.96  val loss: 1.26 valid acc: 0.65\n",
      "i=500   train loss: 1.24 train acc: 0.96  val loss: 1.13 valid acc: 0.74\n",
      "i=600   train loss: 1.18 train acc: 0.93  val loss: 1.07 valid acc: 0.77\n",
      "i=700   train loss: 1.07 train acc: 0.96  val loss: 1.02 valid acc: 0.75\n",
      "i=800   train loss: 1.02 train acc: 0.96  val loss: 0.99 valid acc: 0.75\n",
      "i=900   train loss: 0.96 train acc: 0.96  val loss: 0.93 valid acc: 0.79\n",
      "i=1000  train loss: 0.94 train acc: 0.96  val loss: 0.91 valid acc: 0.78\n",
      "i=1100  train loss: 0.91 train acc: 0.95  val loss: 0.90 valid acc: 0.78\n",
      "i=1200  train loss: 0.85 train acc: 0.96  val loss: 0.98 valid acc: 0.80\n",
      "i=1300  train loss: 0.90 train acc: 0.92  val loss: 0.86 valid acc: 0.81\n",
      "i=1400  train loss: 0.82 train acc: 0.96  val loss: 0.93 valid acc: 0.82\n",
      "i=1500  train loss: 0.81 train acc: 0.97  val loss: 0.87 valid acc: 0.79\n",
      "i=1600  train loss: 0.79 train acc: 0.97  val loss: 0.83 valid acc: 0.82\n",
      "i=1700  train loss: 0.77 train acc: 0.97  val loss: 0.89 valid acc: 0.82\n",
      "i=1800  train loss: 0.75 train acc: 0.98  val loss: 0.87 valid acc: 0.82\n",
      "i=1900  train loss: 0.77 train acc: 0.96  val loss: 0.83 valid acc: 0.84\n",
      "i=2000  train loss: 0.75 train acc: 0.97  val loss: 0.93 valid acc: 0.82\n",
      "i=2100  train loss: 0.76 train acc: 0.95  val loss: 0.92 valid acc: 0.83\n",
      "i=2200  train loss: 0.72 train acc: 0.97  val loss: 0.85 valid acc: 0.83\n",
      "i=2300  train loss: 0.71 train acc: 0.97  val loss: 0.86 valid acc: 0.84\n",
      "i=2400  train loss: 0.70 train acc: 0.96  val loss: 0.95 valid acc: 0.84\n",
      "i=2500  train loss: 0.70 train acc: 0.98  val loss: 0.80 valid acc: 0.82\n",
      "i=2600  train loss: 0.68 train acc: 0.97  val loss: 0.94 valid acc: 0.85\n",
      "i=2700  train loss: 0.69 train acc: 0.97  val loss: 0.92 valid acc: 0.83\n",
      "i=2800  train loss: 0.68 train acc: 0.97  val loss: 0.89 valid acc: 0.84\n",
      "i=2900  train loss: 0.68 train acc: 0.97  val loss: 0.96 valid acc: 0.83\n",
      "i=3000  train loss: 0.67 train acc: 0.97  val loss: 0.88 valid acc: 0.84\n",
      "i=3100  train loss: 0.79 train acc: 0.97  val loss: 0.83 valid acc: 0.85\n",
      "i=3200  train loss: 0.66 train acc: 0.97  val loss: 0.92 valid acc: 0.82\n",
      "i=3300  train loss: 0.64 train acc: 0.97  val loss: 0.89 valid acc: 0.81\n",
      "i=3400  train loss: 0.64 train acc: 0.96  val loss: 0.90 valid acc: 0.82\n",
      "i=3500  train loss: 0.64 train acc: 0.98  val loss: 0.94 valid acc: 0.83\n",
      "i=3600  train loss: 0.64 train acc: 0.98  val loss: 0.87 valid acc: 0.85\n",
      "i=3700  train loss: 0.62 train acc: 0.98  val loss: 0.90 valid acc: 0.85\n",
      "i=3800  train loss: 0.62 train acc: 0.97  val loss: 0.91 valid acc: 0.83\n",
      "i=3900  train loss: 0.62 train acc: 0.98  val loss: 0.99 valid acc: 0.84\n",
      "i=4000  train loss: 0.60 train acc: 0.97  val loss: 0.96 valid acc: 0.82\n",
      "i=4100  train loss: 0.63 train acc: 0.96  val loss: 1.01 valid acc: 0.82\n",
      "i=4200  train loss: 0.60 train acc: 0.97  val loss: 0.97 valid acc: 0.83\n",
      "i=4300  train loss: 0.62 train acc: 0.97  val loss: 0.87 valid acc: 0.81\n",
      "i=4400  train loss: 0.60 train acc: 0.97  val loss: 0.90 valid acc: 0.83\n",
      "i=4500  train loss: 0.60 train acc: 0.96  val loss: 1.02 valid acc: 0.82\n",
      "i=4600  train loss: 0.62 train acc: 0.97  val loss: 1.04 valid acc: 0.83\n",
      "i=4700  train loss: 0.59 train acc: 0.98  val loss: 0.90 valid acc: 0.83\n",
      "i=4800  train loss: 0.58 train acc: 0.98  val loss: 0.90 valid acc: 0.84\n",
      "i=4900  train loss: 0.59 train acc: 0.97  val loss: 0.94 valid acc: 0.83\n",
      "i=5000  train loss: 0.57 train acc: 0.98  val loss: 0.89 valid acc: 0.83\n",
      "i=5100  train loss: 0.59 train acc: 0.97  val loss: 0.87 valid acc: 0.83\n",
      "i=5200  train loss: 0.59 train acc: 0.96  val loss: 0.96 valid acc: 0.83\n",
      "i=5300  train loss: 0.57 train acc: 0.98  val loss: 0.91 valid acc: 0.83\n",
      "i=5400  train loss: 0.59 train acc: 0.97  val loss: 0.90 valid acc: 0.82\n",
      "i=5500  train loss: 0.57 train acc: 0.97  val loss: 0.84 valid acc: 0.84\n",
      "i=5600  train loss: 0.58 train acc: 0.96  val loss: 0.92 valid acc: 0.82\n",
      "i=5700  train loss: 0.58 train acc: 0.97  val loss: 0.87 valid acc: 0.81\n",
      "i=5800  train loss: 0.57 train acc: 0.98  val loss: 0.95 valid acc: 0.84\n",
      "i=5900  train loss: 0.56 train acc: 0.98  val loss: 0.95 valid acc: 0.83\n",
      "i=6000  train loss: 0.57 train acc: 0.97  val loss: 1.01 valid acc: 0.83\n",
      "i=6100  train loss: 0.56 train acc: 0.98  val loss: 0.86 valid acc: 0.83\n",
      "i=6200  train loss: 0.57 train acc: 0.97  val loss: 1.05 valid acc: 0.82\n",
      "i=6300  train loss: 0.57 train acc: 0.98  val loss: 0.96 valid acc: 0.81\n",
      "i=6400  train loss: 0.55 train acc: 0.98  val loss: 0.85 valid acc: 0.83\n",
      "i=6500  train loss: 0.54 train acc: 0.98  val loss: 0.80 valid acc: 0.82\n",
      "i=6600  train loss: 0.56 train acc: 0.97  val loss: 1.01 valid acc: 0.84\n",
      "i=6700  train loss: 0.54 train acc: 0.98  val loss: 0.99 valid acc: 0.82\n",
      "i=6800  train loss: 0.54 train acc: 0.98  val loss: 0.91 valid acc: 0.82\n",
      "i=6900  train loss: 0.57 train acc: 0.96  val loss: 0.98 valid acc: 0.83\n",
      "i=7000  train loss: 0.56 train acc: 0.97  val loss: 0.90 valid acc: 0.83\n",
      "i=7100  train loss: 0.58 train acc: 0.96  val loss: 0.98 valid acc: 0.80\n",
      "i=7200  train loss: 0.55 train acc: 0.97  val loss: 1.01 valid acc: 0.82\n",
      "i=7300  train loss: 0.56 train acc: 0.97  val loss: 0.99 valid acc: 0.81\n",
      "i=7400  train loss: 0.52 train acc: 0.98  val loss: 0.93 valid acc: 0.82\n",
      "i=7500  train loss: 0.54 train acc: 0.97  val loss: 0.90 valid acc: 0.83\n",
      "i=7600  train loss: 0.54 train acc: 0.97  val loss: 1.04 valid acc: 0.82\n",
      "i=7700  train loss: 0.54 train acc: 0.97  val loss: 0.96 valid acc: 0.83\n",
      "i=7800  train loss: 0.52 train acc: 0.98  val loss: 0.94 valid acc: 0.83\n",
      "i=7900  train loss: 0.52 train acc: 0.98  val loss: 0.99 valid acc: 0.80\n",
      "i=8000  train loss: 0.53 train acc: 0.97  val loss: 1.00 valid acc: 0.82\n",
      "i=8100  train loss: 0.52 train acc: 0.98  val loss: 1.02 valid acc: 0.83\n",
      "i=8200  train loss: 0.54 train acc: 0.97  val loss: 0.97 valid acc: 0.81\n",
      "i=8300  train loss: 0.54 train acc: 0.97  val loss: 1.04 valid acc: 0.80\n",
      "i=8400  train loss: 0.57 train acc: 0.98  val loss: 0.91 valid acc: 0.80\n",
      "i=8500  train loss: 0.56 train acc: 0.95  val loss: 0.99 valid acc: 0.82\n",
      "i=8600  train loss: 0.53 train acc: 0.97  val loss: 1.05 valid acc: 0.81\n",
      "i=8700  train loss: 0.52 train acc: 0.97  val loss: 1.02 valid acc: 0.82\n",
      "i=8800  train loss: 0.52 train acc: 0.98  val loss: 1.07 valid acc: 0.80\n",
      "i=8900  train loss: 0.51 train acc: 0.98  val loss: 0.97 valid acc: 0.81\n",
      "i=9000  train loss: 0.51 train acc: 0.98  val loss: 0.99 valid acc: 0.82\n",
      "i=9100  train loss: 0.53 train acc: 0.98  val loss: 0.91 valid acc: 0.83\n",
      "i=9200  train loss: 0.50 train acc: 0.99  val loss: 1.06 valid acc: 0.81\n",
      "i=9300  train loss: 0.52 train acc: 0.98  val loss: 0.97 valid acc: 0.82\n",
      "i=9400  train loss: 0.48 train acc: 0.98  val loss: 0.99 valid acc: 0.80\n",
      "i=9500  train loss: 0.51 train acc: 0.98  val loss: 0.99 valid acc: 0.83\n",
      "i=9600  train loss: 0.51 train acc: 0.98  val loss: 1.01 valid acc: 0.81\n",
      "i=9700  train loss: 0.52 train acc: 0.98  val loss: 0.91 valid acc: 0.80\n",
      "i=9800  train loss: 0.52 train acc: 0.98  val loss: 0.88 valid acc: 0.81\n",
      "i=9900  train loss: 0.50 train acc: 0.98  val loss: 1.08 valid acc: 0.82\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7b4de01dda90ec36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
